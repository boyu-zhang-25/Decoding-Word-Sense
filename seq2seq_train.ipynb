{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "from io import open\n",
    "from conllu import parse_incr\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Iterable, defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the WSD dataset first\n",
    "# and retrieve all sentences from the EUD\n",
    "\n",
    "'''\n",
    "Copyright@\n",
    "White, A. S., D. Reisinger, K. Sakaguchi, T. Vieira, S. Zhang, R. Rudinger, K. Rawlins, & B. Van Durme. 2016. \n",
    "[Universal decompositional semantics on universal dependencies]\n",
    "(http://aswhite.net/media/papers/white_universal_2016.pdf). \n",
    "To appear in *Proceedings of the Conference on Empirical Methods in Natural Language Processing 2016*.\n",
    "'''\n",
    "\n",
    "def parse_wsd_data():\n",
    "\n",
    "    # parse the EUD-EWT conllu files and retrieve the sentences\n",
    "    # remove all punctuation?\n",
    "    train_file = open(\"data/UD_English-EWT/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    train_data = list(parse_incr(train_file))\n",
    "    # train_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in train_data]\n",
    "    # train_data = [[word for word in s if word] for s in train_data]\n",
    "    print('Parsed {} training data from UD_English-EWT/en_ewt-ud-train.conllu.'.format(len(train_data)))\n",
    "\n",
    "    test_file = open(\"data/UD_English-EWT/en_ewt-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    test_data = list(parse_incr(test_file))\n",
    "    # test_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in test_data]\n",
    "    # test_data = [[word for word in s if word] for s in test_data]\n",
    "    print('Parsed {} testing data from UD_English-EWT/en_ewt-ud-test.conllu.'.format(len(test_data)))\n",
    "\n",
    "    dev_file = open(\"data/UD_English-EWT/en_ewt-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    dev_data = list(parse_incr(dev_file))\n",
    "    # dev_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in dev_data]\n",
    "    # dev_data = [[word for word in s if word] for s in dev_data]\n",
    "    print('Parsed {} dev data from UD_English-EWT/en_ewt-ud-dev.conllu.'.format(len(dev_data)))\n",
    "\n",
    "    # parse the WSD dataset\n",
    "    wsd_data = []\n",
    "\n",
    "    # read in tsv by White et. al., 2016\n",
    "    with open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "\n",
    "        tsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')      \n",
    "\n",
    "        # store the data: ordered dict row\n",
    "        for row in tsv_reader:                                \n",
    "                            \n",
    "            # each data vector\n",
    "            wsd_data.append(row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print('Parsed {} word sense data from White et. al., 2016.'.format(len(wsd_data)))\n",
    "\n",
    "    return wsd_data, train_data, test_data, dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 12543 training data from UD_English-EWT/en_ewt-ud-train.conllu.\n",
      "Parsed 2077 testing data from UD_English-EWT/en_ewt-ud-test.conllu.\n",
      "Parsed 2002 dev data from UD_English-EWT/en_ewt-ud-dev.conllu.\n",
      "Parsed 439312 word sense data from White et. al., 2016.\n"
     ]
    }
   ],
   "source": [
    "# get the raw wsd data\n",
    "wsd_data, train_data, test_data, dev_data = parse_wsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return: \n",
    "all senses for each word \n",
    "all definitions for each word\n",
    "from the EUD for train, test, and dev dataset\n",
    "index provided by WSD dataset by White et. al.\n",
    "'''\n",
    "# get all the senses and definitions for each word from WSD dataset\n",
    "# order of senses and definitions are in order\n",
    "def get_all_senses_and_definitions(wsd_data, train_data, test_data, dev_data):\n",
    "\n",
    "    # all senses for each word in train and dev\n",
    "    all_senses = {}\n",
    "    all_definitions = {}\n",
    "\n",
    "    # all senses for each word in test\n",
    "    all_test_senses = {}\n",
    "    all_test_definitions = {}\n",
    "    \n",
    "    # only get the senses for train and dev set\n",
    "    for i in range(len(wsd_data)):\n",
    "        \n",
    "        # get the original sentence from EUD\n",
    "        sentence_id = wsd_data[i].get('Sentence.ID')\n",
    "        \n",
    "        # get the definitions for the target word from EUD\n",
    "        definition = wsd_data[i].get('Sense.Definition').split(' ')\n",
    "        \n",
    "        # the index in EUD is 1-based!!!\n",
    "        sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "        word_index = int(wsd_data[i].get('Arg.Token')) - 1\n",
    "        \n",
    "        word_lemma = wsd_data[i].get('Arg.Lemma')\n",
    "        word_sense = wsd_data[i].get('Synset')\n",
    "        response = wsd_data[i].get('Sense.Response')\n",
    "        \n",
    "        # add a under score to avoid name conflict with pytorch build-in attributes\n",
    "        # get the original word\n",
    "        # in case of errors in the dataset\n",
    "        # correct it to the original word the annotator saw\n",
    "        # add a under score to avoid name conflict with pytorch build-in attributes\n",
    "        old = '____' + word_lemma\n",
    "        if wsd_data[i].get('Split') == 'train':\n",
    "            sentence = train_data[sentence_number]\n",
    "            word_lemma = '____' + [word.get('lemma') for word in sentence][word_index]\n",
    "        elif wsd_data[i].get('Split') == 'test':\n",
    "            sentence = test_data[sentence_number]\n",
    "            word_lemma = '____' + [word.get('lemma') for word in sentence][word_index]\n",
    "        else:\n",
    "            sentence = dev_data[sentence_number]\n",
    "            word_lemma = '____' + [word.get('lemma') for word in sentence][word_index]\n",
    "        \n",
    "        # index error in UD: some sentences start with '<<'\n",
    "        # have wrong index\n",
    "        if [word.get('lemma') for word in sentence][0] == '<<' and [word.get('lemma') for word in sentence][-1] != '>>':\n",
    "            if '____' + [word.get('lemma') for word in sentence][word_index] != old:\n",
    "                word_lemma = old\n",
    "                \n",
    "        # senses for train and dev\n",
    "        # preserve unknown words\n",
    "        if wsd_data[i].get('Split') != 'test':\n",
    "\n",
    "            # if the word already exits: add the new sense to the list\n",
    "            # else: creata a new list for the word\n",
    "            if all_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "                if word_sense not in all_senses[word_lemma]:\n",
    "                    all_senses[word_lemma].append(word_sense)\n",
    "            else:\n",
    "                all_senses[word_lemma] = []\n",
    "                all_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "            if all_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "                if definition not in all_definitions[word_lemma]: \n",
    "                    all_definitions[word_lemma].append(definition)\n",
    "            else:\n",
    "                all_definitions[word_lemma] = []\n",
    "                all_definitions[word_lemma].append(definition)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            # all the senses and definitions for test words\n",
    "            if all_test_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "                if word_sense not in all_test_senses[word_lemma]:\n",
    "                    all_test_senses[word_lemma].append(word_sense)\n",
    "            else:\n",
    "                all_test_senses[word_lemma] = []\n",
    "                all_test_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "            if all_test_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "                if definition not in all_test_definitions[word_lemma]: \n",
    "                    all_test_definitions[word_lemma].append(definition)\n",
    "            else:\n",
    "                all_test_definitions[word_lemma] = []\n",
    "                all_test_definitions[word_lemma].append(definition)            \n",
    "    \n",
    "    return all_senses, all_definitions, all_test_senses, all_test_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the senses and definitions\n",
    "all_senses, all_definitions, all_test_senses, all_test_definitions = get_all_senses_and_definitions(wsd_data, train_data, test_data, dev_data)\n",
    "\n",
    "# debug: make sure every word is in the WN\n",
    "error_set = set()\n",
    "for se in all_senses.keys():\n",
    "    se = se.split('____')[-1]\n",
    "    if len(wn.synsets(se)) == 0:\n",
    "        error_set.add(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train, dev, test datasets from processed files\n",
    "# check the 'data_loader.ipynb' for details\n",
    "def read_file():\n",
    "    \n",
    "    train_X = []\n",
    "    train_X_num = 0\n",
    "    train_Y = []\n",
    "    train_Y_num = 0\n",
    "    test_X = []\n",
    "    test_X_num = 0\n",
    "    test_Y = []\n",
    "    test_Y_num = 0\n",
    "    dev_X = []\n",
    "    dev_X_num = 0\n",
    "    dev_Y = []\n",
    "    dev_Y_num = 0\n",
    "    \n",
    "    train_word_idx = []\n",
    "    test_word_idx = []\n",
    "    dev_word_idx = []\n",
    "    \n",
    "    # debug: make sure every word is in the WordNet\n",
    "    WN_set = set(wn.all_synsets())\n",
    "    \n",
    "    # read in csv\n",
    "    with open('data/train_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            train_X.append(row)\n",
    "            train_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_X_num} data points for train_X.')\n",
    "\n",
    "    with open('data/train_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_Y.append(row)\n",
    "            train_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_Y_num} data points for train_Y.')\n",
    "        \n",
    "    with open('data/train_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(train_word_idx)} data points for train_word_idx.')\n",
    "\n",
    "    with open('data/dev_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            dev_X.append(row)\n",
    "            dev_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_X_num} data points for dev_X.')\n",
    "\n",
    "    with open('data/dev_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_Y.append(row)\n",
    "            dev_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_Y_num} data points for dev_Y.')\n",
    "        \n",
    "    with open('data/dev_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(dev_word_idx)} data points for dev_word_idx.')\n",
    "        \n",
    "    with open('data/test_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            test_X.append(row)\n",
    "            test_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_X_num} data points for test_X.')\n",
    "\n",
    "    with open('data/test_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_Y.append(row)\n",
    "            test_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_Y_num} data points for test_Y.')\n",
    "        \n",
    "    with open('data/test_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(test_word_idx)} data points for test_word_idx.')    \n",
    "        \n",
    "    return train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the structured data\n",
    "train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx = read_file()\n",
    "\n",
    "# debug: make sure every word is in the WN\n",
    "for i, j in enumerate(train_X):\n",
    "    if j[train_word_idx[i]] in error_set:\n",
    "        print(j[train_word_idx[i]])\n",
    "        del train_X[i]\n",
    "        del train_Y[i]\n",
    "        del train_word_idx[i]\n",
    "# debug: make sure every word is in the WN\n",
    "for i, j in enumerate(test_X):\n",
    "    if j[test_word_idx[i]] in error_set:\n",
    "        print(j[test_word_idx[i]])\n",
    "        del test_X[i]\n",
    "        del test_Y[i]\n",
    "        del test_word_idx[i]\n",
    "# debug: make sure every word is in the WN\n",
    "for i, j in enumerate(dev_X):\n",
    "    if j[dev_word_idx[i]] in error_set:\n",
    "        print(j[dev_word_idx[i]])\n",
    "        del dev_X[i]\n",
    "        del dev_Y[i]\n",
    "        del dev_word_idx[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 67619 data points for train_X.\n",
      "Parsed 67619 data points for train_Y.\n",
      "Parsed 67619 data points for train_word_idx.\n",
      "Parsed 7332 data points for dev_X.\n",
      "Parsed 7332 data points for dev_Y.\n",
      "Parsed 7332 data points for dev_word_idx.\n",
      "Parsed 7118 data points for test_X.\n",
      "Parsed 7118 data points for test_Y.\n",
      "Parsed 7118 data points for test_word_idx.\n",
      "entrée\n",
      "we\n",
      "we\n",
      "of\n",
      "my\n",
      "entrée\n",
      "my\n",
      "of\n",
      "we\n",
      "entrée\n",
      "my\n",
      "we\n",
      "we\n",
      "of\n",
      "£\n",
      "£\n",
      "number of known words: 1872\n",
      "number of train data: 10000\n",
      "number of dev data: 2000\n",
      "number of test data: 7118\n"
     ]
    }
   ],
   "source": [
    "# test on small subset\n",
    "train_X = train_X[:10000]\n",
    "train_Y = train_Y[:10000]\n",
    "train_word_idx = train_word_idx[:10000]\n",
    "\n",
    "# small vocab from the first 10000 sentences\n",
    "vocab = set()\n",
    "for index, sen in enumerate(train_X):\n",
    "    word = sen[train_word_idx[index]]\n",
    "    if word not in vocab and ('____' + word) in all_senses.keys():\n",
    "        vocab.add(word)\n",
    "print('number of known words: {}'.format(len(vocab)))\n",
    "print('number of train data: {}'.format(len(train_X)))\n",
    "\n",
    "# filter the dev set\n",
    "new_dev_X = []\n",
    "new_dev_Y = []\n",
    "new_dev_idx = []\n",
    "for index, sen in enumerate(dev_X):\n",
    "    word = sen[dev_word_idx[index]]\n",
    "    if word in vocab and ('____' + word) in all_senses.keys():\n",
    "        new_dev_idx.append(dev_word_idx[index])\n",
    "        new_dev_X.append(sen)\n",
    "        new_dev_Y.append(dev_Y[index])\n",
    "new_dev_X = new_dev_X[:2000]\n",
    "new_dev_Y = new_dev_Y[:2000]\n",
    "new_dev_idx = new_dev_idx[:2000]\n",
    "\n",
    "print('number of dev data: {}'.format(len(new_dev_X)))\n",
    "\n",
    "# filter the test set\n",
    "new_test_idx = []\n",
    "new_test_X = []\n",
    "new_test_Y = []\n",
    "for index, sen in enumerate(test_X):\n",
    "    word = sen[test_word_idx[index]]\n",
    "    if word in vocab and ('____' + word) in all_senses.keys():\n",
    "        new_test_idx.append(test_word_idx[index])\n",
    "        new_test_X.append(sen)\n",
    "        new_test_Y.append(test_Y[index]) \n",
    "    elif word not in all_senses.keys():\n",
    "        new_test_idx.append(test_word_idx[index])\n",
    "        new_test_X.append(sen)\n",
    "        new_test_Y.append(test_Y[index])        \n",
    "            \n",
    "print('number of test data: {}'.format(len(new_test_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()\n",
    "\n",
    "from encoder import *\n",
    "from decoder import *\n",
    "from seq2seq_model import *\n",
    "\n",
    "# get the decoder vocab\n",
    "with open('./data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq_Model(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleDict(\n",
       "      (word_sense): ModuleList(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=300, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (mlp_dropout): Dropout(p=0)\n",
       "    (dimension_reduction_MLP): Linear(in_features=3072, out_features=256, bias=True)\n",
       "    (lstm): LSTM(256, 256, num_layers=2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lstm): LSTM(300, 300)\n",
       "    (linear): Linear(in_features=300, out_features=49036, bias=True)\n",
       "  )\n",
       "  (embed): Embedding(49036, 300)\n",
       "  (dropout): Dropout(p=0)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size = vocab.idx)\n",
    "encoder = Encoder(all_senses = all_senses, elmo_class = elmo)\n",
    "seq2seq_model = Seq2Seq_Model(encoder, decoder, all_senses = all_senses)\n",
    "\n",
    "# randomly initialize the weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)       \n",
    "seq2seq_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# training hyperparameters\n",
    "epochs = 0\n",
    "optimizer = optim.Adam(seq2seq_model.parameters())\n",
    "\n",
    "PAD_IDX = vocab('<pad>')\n",
    "print(PAD_IDX)\n",
    "loss = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# train_losses, dev_losses, dev_rs = trainer.train(train_X, train_Y, train_word_idx, dev_X, dev_Y, dev_word_idx)\n",
    "\n",
    "# small vocab\n",
    "train_losses, dev_losses, dev_rs = trainer.train(train_X, train_Y, train_word_idx, new_dev_X, new_dev_Y, new_dev_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "with open('train_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(train_losses)\n",
    "\n",
    "    \n",
    "with open('dev_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(dev_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(train_losses, ms = 4, marker = 's', label = \"Train Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(train_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(dev_losses, ms = 4, marker = 'o', label = \"Dev Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(dev_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dev_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "print('test debug')\n",
    "for test_idx, test_sen in enumerate(test_X):\n",
    "    test_lemma = '____' + test_sen[test_word_idx[test_idx]]\n",
    "    emb_length = len(all_test_senses.get(test_lemma))\n",
    "    y = len(test_Y[test_idx])\n",
    "    \n",
    "    if emb_length != y:\n",
    "        print('lemma: {}, y: {}, emb: {}'.format(test_lemma, y, emb_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "print('dev debug')\n",
    "for test_idx, test_sen in enumerate(dev_X):\n",
    "    test_lemma = '____' + test_sen[dev_word_idx[test_idx]]\n",
    "    emb_length = len(all_senses.get(test_lemma))\n",
    "    y = len(dev_Y[test_idx])\n",
    "    \n",
    "    if emb_length != y:\n",
    "        print('lemma: {}, y: {}, emb: {}'.format(test_lemma, y, emb_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# should print nothing \n",
    "print('train debug')\n",
    "for test_idx, test_sen in enumerate(train_X):\n",
    "    test_lemma = '____' + test_sen[train_word_idx[test_idx]]\n",
    "    if all_senses.get(test_lemma, 'e') == 'e':\n",
    "        print(test_lemma)\n",
    "        print(test_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# should print nothing \n",
    "print('test debug')\n",
    "for test_idx, test_sen in enumerate(test_X):\n",
    "    test_lemma = '____' + test_sen[test_word_idx[test_idx]]\n",
    "    if all_test_senses.get(test_lemma, 'e') == 'e':\n",
    "        print(test_lemma)\n",
    "        print(test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "cos = nn.CosineSimilarity(dim = 1, eps = 1e-6).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "correct_count = 0\n",
    "known_test_size = 0\n",
    "unknown_test_size = 0\n",
    "unknown_correct_count = 0\n",
    "\n",
    "embds = []\n",
    "\n",
    "new_test_idx = new_test_idx[:100]\n",
    "new_test_X = new_test_X[:100]\n",
    "new_test_Y = new_test_Y[:100]\n",
    "\n",
    "# overall accuracy\n",
    "for test_idx, test_sen in enumerate(new_test_X):\n",
    "    \n",
    "    test_lemma = '____' + test_sen[new_test_idx[test_idx]]\n",
    "        \n",
    "    # print(test_sen)\n",
    "    test_emb = trainer._model.forward(test_sen, new_test_idx[test_idx]).view(1, -1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    # print(test_emb)\n",
    "    all_similarity = []\n",
    "    # embds.append(test_emb)\n",
    "    \n",
    "    # if it is a new word\n",
    "    # only test on the supersense\n",
    "    if all_senses.get(test_lemma, 'e') == 'e':\n",
    "        \n",
    "        unknown_test_size += 1\n",
    "        test_result = ''\n",
    "        best_sim = -float('inf')\n",
    "        \n",
    "        for n, new_s in enumerate(all_test_senses[test_lemma]):\n",
    "            \n",
    "            new_super = wn.synset(new_s).lexname().replace('.', '_')\n",
    "            super_vec = trainer._model.supersense_embeddings[new_super].view(1, -1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            cos_sim = cos(test_emb, super_vec)\n",
    "            \n",
    "            if cos_sim > best_sim:\n",
    "                test_result = new_super\n",
    "                best_sim = cos_sim\n",
    "                \n",
    "        correct_super = []\n",
    "        for q, respon in enumerate(new_test_Y[test_idx]):\n",
    "            if respon:\n",
    "                correct_s = wn.synset(all_test_senses[test_lemma][q]).lexname().replace('.', '_')\n",
    "                correct_super.append(correct_s)            \n",
    "        if test_result in correct_super:\n",
    "            unknown_correct_count += 1\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        # if it is a known word\n",
    "        known_test_size += 1\n",
    "        \n",
    "        for k, sense in enumerate(all_senses[test_lemma]):\n",
    "            definition_vec = trainer._model.definition_embeddings[test_lemma][:, k].view(1, -1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            cos_sim = cos(test_emb, definition_vec)\n",
    "            all_similarity.append(cos_sim)\n",
    "        # print(all_similarity)\n",
    "        test_result = all_similarity.index(max(all_similarity))\n",
    "        # print(\"result index: {}\".format(test_result))\n",
    "        if new_test_Y[test_idx][test_result] == 1:\n",
    "            correct_count += 1\n",
    "\n",
    "print('test size for known words: {}'.format(known_test_size))\n",
    "print('accuracy for known words: {}'.format(correct_count / known_test_size))\n",
    "\n",
    "print('test size for unknown words: {}'.format(unknown_test_size))\n",
    "print('accuracy for unknown words: {}'.format(unknown_correct_count / unknown_test_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
