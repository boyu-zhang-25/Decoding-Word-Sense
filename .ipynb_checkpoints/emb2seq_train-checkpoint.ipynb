{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "from io import open\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Iterable, defaultdict\n",
    "import random\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import semcor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set determinstic results\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Device: cpu\n",
      "0\n",
      "Size of vocab: 49036\n"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()\n",
    "\n",
    "from encoder import *\n",
    "from decoder import *\n",
    "from emb2seq_model import *\n",
    "\n",
    "# get the decoder vocab\n",
    "with open('./data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    print(\"Size of vocab: {}\".format(vocab.idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emb2Seq_Model(\n",
       "  (encoder): Encoder(\n",
       "    (dimension_reduction): Linear(in_features=3072, out_features=512, bias=True)\n",
       "    (lstm): LSTM(512, 256, num_layers=2, bidirectional=True)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=300, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0)\n",
       "      (3): Linear(in_features=300, out_features=256, bias=True)\n",
       "      (4): Dropout(p=0)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lstm_cell): LSTMCell(512, 256)\n",
       "    (linear): Linear(in_features=256, out_features=49036, bias=True)\n",
       "  )\n",
       "  (embed): Embedding(49036, 256, padding_idx=0)\n",
       "  (dropout): Dropout(p=0)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size = vocab.idx)\n",
    "encoder = Encoder(elmo_class = elmo)\n",
    "emb2seq_model = Emb2Seq_Model(encoder, decoder, vocab = vocab)\n",
    "\n",
    "# randomly initialize the weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)       \n",
    "emb2seq_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD_IDX: 0\n"
     ]
    }
   ],
   "source": [
    "# training hyperparameters\n",
    "optimizer = optim.Adam(emb2seq_model.parameters())\n",
    "PAD_IDX = vocab('<pad>')\n",
    "print('PAD_IDX: {}'.format(PAD_IDX))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "# turn the given definition into its index list form\n",
    "def def2idx(definition, max_length, vocab):\n",
    "    \n",
    "    # definition is given by the WN NLTK API in a string\n",
    "    def_tokens = nltk.tokenize.word_tokenize(definition.lower())\n",
    "    \n",
    "    # limit the length if too long, trim\n",
    "    if len(def_tokens) > (max_length - 2):\n",
    "        def_tokens = def_tokens[0:(max_length - 2)]\n",
    "        \n",
    "        # add the start and end symbol\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "    \n",
    "    # if the length is too short, pad\n",
    "    elif len(def_tokens) < (max_length - 2):\n",
    "        \n",
    "        # add the start and end symbol\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "        \n",
    "        pad = ['<pad>'] * (max_length - len(def_tokens))\n",
    "        def_tokens = def_tokens + pad\n",
    "        \n",
    "    else:\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "            \n",
    "    # get the index for each element in the token list\n",
    "    def_idx_list = [vocab(token) for token in def_tokens]\n",
    "    \n",
    "    return def_idx_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_size = 10\n",
    "small_dev_size = 5\n",
    "\n",
    "# the training function\n",
    "def train(model, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # SGD\n",
    "    # small test\n",
    "    for idx in range(small_train_size):\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the semcor tagged sentence\n",
    "        sentence = semcor.sents()[idx]\n",
    "        tagged_sent = semcor.tagged_sents(tag = 'sem')[idx]\n",
    "        # print(idx)\n",
    "        \n",
    "        # get all-word definitions\n",
    "        # [batch_size, self.max_length]\n",
    "        definitions = []\n",
    "        for chunk in tagged_sent:\n",
    "            \n",
    "            # if is tagged\n",
    "            if isinstance(chunk, Tree):\n",
    "                \n",
    "                # if is an ambiguous word\n",
    "                if isinstance(chunk.label(), nltk.corpus.reader.wordnet.Lemma):\n",
    "                    # print(chunk.label())\n",
    "                    synset = chunk.label().synset().name()\n",
    "                    definition = wn.synset(synset).definition()\n",
    "                    def_idx_list = def2idx(definition, model.max_length, vocab)\n",
    "                    definitions.append(def_idx_list)\n",
    "        \n",
    "        # get the encoder-decoder result\n",
    "        # (self.max_length, batch_size, vocab_size)\n",
    "        output, result = model(sentence, tagged_sent, definitions, teacher_forcing_ratio = 0.4)\n",
    "        \n",
    "        # adjust dimension for loss calculation\n",
    "        output = output.permute(1, 2, 0)\n",
    "        # print(output.shape)\n",
    "        target = torch.tensor(definitions, dtype = torch.long)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # add clip for gradient boost\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / small_test_size, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "def evaluate(model, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for idx in range(small_dev_size):\n",
    "\n",
    "            # get the semcor tagged sentence\n",
    "            sentence = semcor.sents()[idx + small_train_size]\n",
    "            tagged_sent = semcor.tagged_sents(tag = 'sem')[idx + small_train_size]\n",
    "            print(idx)\n",
    "\n",
    "            # get all-word definitions\n",
    "            # [batch_size, self.max_length]\n",
    "            definitions = []\n",
    "            for idx, chunk in enumerate(tagged_sent):\n",
    "                if isinstance(chunk, Tree):\n",
    "\n",
    "                    # only take in ambiguous words\n",
    "                    if isinstance(chunk.label(), nltk.corpus.reader.wordnet.Lemma):\n",
    "                        # print(chunk.label())\n",
    "                        synset = chunk.label().synset().name()\n",
    "                        definition = wn.synset(synset).definition()\n",
    "                        \n",
    "                        print(definition)\n",
    "                        def_idx_list = def2idx(definition, model.max_length, vocab)\n",
    "                        definitions.append(def_idx_list)\n",
    "\n",
    "            # get the encoder-decoder result\n",
    "            # (max_length, batch_size, vocab_size)\n",
    "            # turn off teacher forcing\n",
    "            output, result = model(sentence, tagged_sent, definitions, teacher_forcing_ratio = 0)\n",
    "\n",
    "            # adjust dimension for loss calculation\n",
    "            output = output.permute(1, 2, 0)\n",
    "            # print(output.shape)\n",
    "            target = torch.tensor(definitions, dtype = torch.long)\n",
    "\n",
    "            loss = criterion(output, target)            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / small_dev_size, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time used by each epoch\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "0\n",
      "the act of accomplishing some aim or executing some order\n",
      "a state in southeastern United States; one of the Confederate states during the American Civil War\n",
      "a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "a legal document signed and sealed and delivered to effect a transfer of property and to show the legal right to possess it\n",
      "legal document setting forth rules governing a particular kind of activity\n",
      "in addition\n",
      "push for something\n",
      "leaving a place or a position\n",
      "a body of citizens sworn to give a true verdict according to the evidence presented in a court of law\n",
      "1\n",
      "force or impel in an indicated direction\n",
      "(of elected officers) elected but not yet serving\n",
      "persons who make or amend or repeal laws\n",
      "give something useful or necessary to\n",
      "providing legal power or sanction\n",
      "a reserve of money set aside for some purpose\n",
      "set anew\n",
      "exerting force or influence\n",
      "the specified day of the month\n",
      "the act of accomplishing some aim or executing some order\n",
      "legal document setting forth rules governing a particular kind of activity\n",
      "produce\n",
      "2\n",
      "a jury to inquire into accusations of crime and to evaluate the grounds for indictments\n",
      "any number of entities (members) considered as a unit\n",
      "manual (or mechanical) carrying or moving or delivering or working with something\n",
      "national; especially in reference to the government of the United States as distinct from that of its member units\n",
      "a reserve of money set aside for some purpose\n",
      "an administrative unit responsible for social work concerned with the welfare and vocational training of children\n",
      "a household in which an orphaned or delinquent child is placed (usually by a social-service agency)\n",
      "3\n",
      "be identical to; be someone or something\n",
      "used of a single unit or thing; not two or more\n",
      "of greater importance or stature or rank\n",
      "an isolated fact that is considered separately from the whole\n",
      "a point or extent in space\n",
      "applying to all or most members of a category or group\n",
      "the activity of contributing to the fulfillment of a need or furtherance of an effort or purpose\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "a body of citizens sworn to give a true verdict according to the evidence presented in a court of law\n",
      "report or maintain\n",
      "any number of entities (members) considered as a unit\n",
      "administer or bestow, as in small portions\n",
      "a reserve of money set aside for some purpose\n",
      "governmental provision of economic assistance to persons in need\n",
      "a specialized division of a large organization\n",
      "quantifier; used with either mass or count nouns to indicate the whole number or amount of or every one of a class\n",
      "(United Kingdom) a region created by territorial division for the purpose of local government\n",
      "the territory occupied by one of the constituent administrative districts of a nation\n",
      "a deliberate act of omission\n",
      "a point or extent in space\n",
      "get something; come into possession of\n",
      "the most common medium of exchange; functions as legal tender\n",
      "4\n",
      "someone who serves (or waits to be called to serve) on a jury\n",
      "express in words\n",
      "be fully aware or cognizant of\n",
      "agreeing in amount, magnitude, or degree\n",
      "the spatial or geographic property of being scattered about over a range, area, or volume\n",
      "a reserve of money set aside for some purpose\n",
      "make unable to perform a certain action\n",
      "a series of steps to be carried out or goals to be accomplished\n",
      "(comparative of `little' usually used with mass nouns) a quantifier meaning not as great in amount or degree\n",
      "densely populated\n",
      "(United Kingdom) a region created by territorial division for the purpose of local government\n",
      "[tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])]\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "['<start>', '<start>', '<start>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>', '<end>']\n",
      "Epoch: 01 | Time: 0m 41s\n",
      "\tTrain Loss: 5.921 | Train PPL: 372.675\n",
      "\t Val. Loss: 6.813 |  Val. PPL: 909.150\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, _ = train(emb2seq_model, optimizer, criterion, CLIP)\n",
    "    valid_loss, result = evaluate(emb2seq_model, criterion)\n",
    "    print(result)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # visualize the results\n",
    "    for n in range(len(result[0])):\n",
    "        sense = []\n",
    "        for m in range(len(result)):\n",
    "            w = vocab.idx2word.get(int(result[m][n]))\n",
    "            sense.append(w)\n",
    "        print(sense)\n",
    "    \n",
    "    # save the best model based on the dev set\n",
    "    '''\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq_model.state_dict(), 'best_model.pth')\n",
    "    '''\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "with open('train_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(train_losses)\n",
    "\n",
    "    \n",
    "with open('dev_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(dev_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(train_losses, ms = 4, marker = 's', label = \"Train Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(train_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(dev_losses, ms = 4, marker = 'o', label = \"Dev Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(dev_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dev_loss.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
