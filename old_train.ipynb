{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "from io import open\n",
    "from conllu import parse_incr\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Iterable, defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set determinstic results\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the WSD dataset first\n",
    "# and retrieve all sentences from the EUD\n",
    "\n",
    "'''\n",
    "Copyright@\n",
    "White, A. S., D. Reisinger, K. Sakaguchi, T. Vieira, S. Zhang, R. Rudinger, K. Rawlins, & B. Van Durme. 2016. \n",
    "[Universal decompositional semantics on universal dependencies]\n",
    "(http://aswhite.net/media/papers/white_universal_2016.pdf). \n",
    "To appear in *Proceedings of the Conference on Empirical Methods in Natural Language Processing 2016*.\n",
    "'''\n",
    "\n",
    "def parse_wsd_data():\n",
    "\n",
    "    # parse the EUD-EWT conllu files and retrieve the sentences\n",
    "    # remove all punctuation?\n",
    "    train_file = open(\"data/UD_English-EWT/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    train_data = list(parse_incr(train_file))\n",
    "    # train_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in train_data]\n",
    "    # train_data = [[word for word in s if word] for s in train_data]\n",
    "    print('Parsed {} training data from UD_English-EWT/en_ewt-ud-train.conllu.'.format(len(train_data)))\n",
    "\n",
    "    test_file = open(\"data/UD_English-EWT/en_ewt-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    test_data = list(parse_incr(test_file))\n",
    "    # test_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in test_data]\n",
    "    # test_data = [[word for word in s if word] for s in test_data]\n",
    "    print('Parsed {} testing data from UD_English-EWT/en_ewt-ud-test.conllu.'.format(len(test_data)))\n",
    "\n",
    "    dev_file = open(\"data/UD_English-EWT/en_ewt-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    dev_data = list(parse_incr(dev_file))\n",
    "    # dev_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in dev_data]\n",
    "    # dev_data = [[word for word in s if word] for s in dev_data]\n",
    "    print('Parsed {} dev data from UD_English-EWT/en_ewt-ud-dev.conllu.'.format(len(dev_data)))\n",
    "\n",
    "    # parse the WSD dataset\n",
    "    wsd_data = []\n",
    "\n",
    "    # read in tsv by White et. al., 2016\n",
    "    with open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "\n",
    "        tsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')      \n",
    "\n",
    "        # store the data: ordered dict row\n",
    "        for row in tsv_reader:                                \n",
    "                            \n",
    "            # each data vector\n",
    "            wsd_data.append(row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print('Parsed {} word sense data from White et. al., 2016.'.format(len(wsd_data)))\n",
    "\n",
    "    return wsd_data, train_data, test_data, dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 12543 training data from UD_English-EWT/en_ewt-ud-train.conllu.\n",
      "Parsed 2077 testing data from UD_English-EWT/en_ewt-ud-test.conllu.\n",
      "Parsed 2002 dev data from UD_English-EWT/en_ewt-ud-dev.conllu.\n",
      "Parsed 439312 word sense data from White et. al., 2016.\n"
     ]
    }
   ],
   "source": [
    "# get the raw wsd data\n",
    "wsd_data, train_data, test_data, dev_data = parse_wsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return: \n",
    "all senses for each word \n",
    "all definitions for each word\n",
    "from the EUD for train, test, and dev dataset\n",
    "index provided by WSD dataset by White et. al.\n",
    "'''\n",
    "# get all the senses and definitions for each word from WSD dataset\n",
    "# order of senses and definitions are in order\n",
    "def get_all_senses_and_definitions(wsd_data, train_data, test_data, dev_data):\n",
    "\n",
    "    # all senses for each word in train and dev\n",
    "    all_senses = {}\n",
    "    all_definitions = {}\n",
    "\n",
    "    # all senses for each word in test\n",
    "    all_test_senses = {}\n",
    "    all_test_definitions = {}\n",
    "    \n",
    "    # only get the senses for train and dev set\n",
    "    for i in range(len(wsd_data)):\n",
    "        \n",
    "        # get the original sentence from EUD\n",
    "        sentence_id = wsd_data[i].get('Sentence.ID')\n",
    "        \n",
    "        # get the definitions for the target word from EUD\n",
    "        definition = wsd_data[i].get('Sense.Definition').split(' ')\n",
    "        \n",
    "        # the index in EUD is 1-based!!!\n",
    "        sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "        word_index = int(wsd_data[i].get('Arg.Token')) - 1\n",
    "        \n",
    "        word_lemma = wsd_data[i].get('Arg.Lemma')\n",
    "        word_sense = wsd_data[i].get('Synset')\n",
    "        response = wsd_data[i].get('Sense.Response')\n",
    "        \n",
    "        # add a under score to avoid name conflict with pytorch build-in attributes\n",
    "        # get the original word\n",
    "        # in case of errors in the dataset\n",
    "        # correct it to the original word the annotator saw\n",
    "        # add a under score to avoid name conflict with pytorch build-in attributes\n",
    "        old = '____' + word_lemma\n",
    "        if wsd_data[i].get('Split') == 'train':\n",
    "            sentence = train_data[sentence_number]\n",
    "            word_lemma = '____' + [word.get('lemma') for word in sentence][word_index]\n",
    "        elif wsd_data[i].get('Split') == 'test':\n",
    "            sentence = test_data[sentence_number]\n",
    "            word_lemma = '____' + [word.get('lemma') for word in sentence][word_index]\n",
    "        else:\n",
    "            sentence = dev_data[sentence_number]\n",
    "            word_lemma = '____' + [word.get('lemma') for word in sentence][word_index]\n",
    "        \n",
    "        # index error in UD: some sentences start with '<<'\n",
    "        # have wrong index\n",
    "        if [word.get('lemma') for word in sentence][0] == '<<' and [word.get('lemma') for word in sentence][-1] != '>>':\n",
    "            if '____' + [word.get('lemma') for word in sentence][word_index] != old:\n",
    "                word_lemma = old\n",
    "                \n",
    "        # senses for train and dev\n",
    "        # preserve unknown words\n",
    "        if wsd_data[i].get('Split') != 'test':\n",
    "\n",
    "            # if the word already exits: add the new sense to the list\n",
    "            # else: creata a new list for the word\n",
    "            if all_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "                if word_sense not in all_senses[word_lemma]:\n",
    "                    all_senses[word_lemma].append(word_sense)\n",
    "            else:\n",
    "                all_senses[word_lemma] = []\n",
    "                all_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "            if all_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "                if definition not in all_definitions[word_lemma]: \n",
    "                    all_definitions[word_lemma].append(definition)\n",
    "            else:\n",
    "                all_definitions[word_lemma] = []\n",
    "                all_definitions[word_lemma].append(definition)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            # all the senses and definitions for test words\n",
    "            if all_test_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "                if word_sense not in all_test_senses[word_lemma]:\n",
    "                    all_test_senses[word_lemma].append(word_sense)\n",
    "            else:\n",
    "                all_test_senses[word_lemma] = []\n",
    "                all_test_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "            if all_test_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "                if definition not in all_test_definitions[word_lemma]: \n",
    "                    all_test_definitions[word_lemma].append(definition)\n",
    "            else:\n",
    "                all_test_definitions[word_lemma] = []\n",
    "                all_test_definitions[word_lemma].append(definition)            \n",
    "    \n",
    "    return all_senses, all_definitions, all_test_senses, all_test_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the senses and definitions\n",
    "all_senses, all_definitions, all_test_senses, all_test_definitions = get_all_senses_and_definitions(wsd_data, train_data, test_data, dev_data)\n",
    "\n",
    "# debug: make sure every word is in the WN\n",
    "error_set = set()\n",
    "for se in all_senses.keys():\n",
    "    se = se.split('____')[-1]\n",
    "    if len(wn.synsets(se)) == 0:\n",
    "        error_set.add(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train, dev, test datasets from processed files\n",
    "# check the 'data_loader.ipynb' for details\n",
    "def read_file():\n",
    "    \n",
    "    train_X = []\n",
    "    train_X_num = 0\n",
    "    train_Y = []\n",
    "    train_Y_num = 0\n",
    "    test_X = []\n",
    "    test_X_num = 0\n",
    "    test_Y = []\n",
    "    test_Y_num = 0\n",
    "    dev_X = []\n",
    "    dev_X_num = 0\n",
    "    dev_Y = []\n",
    "    dev_Y_num = 0\n",
    "    \n",
    "    train_word_idx = []\n",
    "    test_word_idx = []\n",
    "    dev_word_idx = []\n",
    "    \n",
    "    # debug: make sure every word is in the WordNet\n",
    "    WN_set = set(wn.all_synsets())\n",
    "    \n",
    "    # read in csv\n",
    "    with open('data/train_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            train_X.append(row)\n",
    "            train_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_X_num} data points for train_X.')\n",
    "\n",
    "    with open('data/train_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_Y.append(row)\n",
    "            train_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_Y_num} data points for train_Y.')\n",
    "        \n",
    "    with open('data/train_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(train_word_idx)} data points for train_word_idx.')\n",
    "\n",
    "    with open('data/dev_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            dev_X.append(row)\n",
    "            dev_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_X_num} data points for dev_X.')\n",
    "\n",
    "    with open('data/dev_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_Y.append(row)\n",
    "            dev_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_Y_num} data points for dev_Y.')\n",
    "        \n",
    "    with open('data/dev_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(dev_word_idx)} data points for dev_word_idx.')\n",
    "        \n",
    "    with open('data/test_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            test_X.append(row)\n",
    "            test_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_X_num} data points for test_X.')\n",
    "\n",
    "    with open('data/test_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_Y.append(row)\n",
    "            test_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_Y_num} data points for test_Y.')\n",
    "        \n",
    "    with open('data/test_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(test_word_idx)} data points for test_word_idx.')    \n",
    "        \n",
    "    return train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 67619 data points for train_X.\n",
      "Parsed 67619 data points for train_Y.\n",
      "Parsed 67619 data points for train_word_idx.\n",
      "Parsed 7332 data points for dev_X.\n",
      "Parsed 7332 data points for dev_Y.\n",
      "Parsed 7332 data points for dev_word_idx.\n",
      "Parsed 7118 data points for test_X.\n",
      "Parsed 7118 data points for test_Y.\n",
      "Parsed 7118 data points for test_word_idx.\n",
      "entrée\n",
      "we\n",
      "we\n",
      "of\n",
      "my\n",
      "entrée\n",
      "my\n",
      "of\n",
      "we\n",
      "entrée\n",
      "my\n",
      "we\n",
      "we\n",
      "of\n",
      "£\n",
      "£\n"
     ]
    }
   ],
   "source": [
    "# get all the structured data\n",
    "train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx = read_file()\n",
    "\n",
    "# debug: make sure every word is in the WN\n",
    "for i, j in enumerate(train_X):\n",
    "    if j[train_word_idx[i]] in error_set:\n",
    "        print(j[train_word_idx[i]])\n",
    "        del train_X[i]\n",
    "        del train_Y[i]\n",
    "        del train_word_idx[i]\n",
    "# debug: make sure every word is in the WN\n",
    "for i, j in enumerate(test_X):\n",
    "    if j[test_word_idx[i]] in error_set:\n",
    "        print(j[test_word_idx[i]])\n",
    "        del test_X[i]\n",
    "        del test_Y[i]\n",
    "        del test_word_idx[i]\n",
    "# debug: make sure every word is in the WN\n",
    "for i, j in enumerate(dev_X):\n",
    "    if j[dev_word_idx[i]] in error_set:\n",
    "        print(j[dev_word_idx[i]])\n",
    "        del dev_X[i]\n",
    "        del dev_Y[i]\n",
    "        del dev_word_idx[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of known words: 6\n",
      "number of train data: 10\n",
      "number of dev data: 12\n",
      "number of test data: 7118\n"
     ]
    }
   ],
   "source": [
    "# test on small subset\n",
    "train_X = train_X[:10]\n",
    "train_Y = train_Y[:10]\n",
    "train_word_idx = train_word_idx[:10]\n",
    "\n",
    "# small vocab from the first 10000 sentences\n",
    "small_vocab = set()\n",
    "for index, sen in enumerate(train_X):\n",
    "    word = sen[train_word_idx[index]]\n",
    "    if word not in small_vocab and ('____' + word) in all_senses.keys():\n",
    "        small_vocab.add(word)\n",
    "print('number of known words: {}'.format(len(small_vocab)))\n",
    "print('number of train data: {}'.format(len(train_X)))\n",
    "\n",
    "# filter the dev set\n",
    "new_dev_X = []\n",
    "new_dev_Y = []\n",
    "new_dev_idx = []\n",
    "for index, sen in enumerate(dev_X):\n",
    "    word = sen[dev_word_idx[index]]\n",
    "    if word in small_vocab and ('____' + word) in all_senses.keys():\n",
    "        new_dev_idx.append(dev_word_idx[index])\n",
    "        new_dev_X.append(sen)\n",
    "        new_dev_Y.append(dev_Y[index])\n",
    "new_dev_X = new_dev_X[:2000]\n",
    "new_dev_Y = new_dev_Y[:2000]\n",
    "new_dev_idx = new_dev_idx[:2000]\n",
    "\n",
    "print('number of dev data: {}'.format(len(new_dev_X)))\n",
    "\n",
    "# filter the test set\n",
    "new_test_idx = []\n",
    "new_test_X = []\n",
    "new_test_Y = []\n",
    "for index, sen in enumerate(test_X):\n",
    "    word = sen[test_word_idx[index]]\n",
    "    if word in small_vocab and ('____' + word) in all_senses.keys():\n",
    "        new_test_idx.append(test_word_idx[index])\n",
    "        new_test_X.append(sen)\n",
    "        new_test_Y.append(test_Y[index]) \n",
    "    elif word not in all_senses.keys():\n",
    "        new_test_idx.append(test_word_idx[index])\n",
    "        new_test_X.append(sen)\n",
    "        new_test_Y.append(test_Y[index])        \n",
    "            \n",
    "print('number of test data: {}'.format(len(new_test_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Device: cpu\n",
      "0\n",
      "Size of vocab: 49036\n"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()\n",
    "\n",
    "from encoder import *\n",
    "from decoder import *\n",
    "from seq2seq_model import *\n",
    "\n",
    "# get the decoder vocab\n",
    "with open('./data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    print(\"Size of vocab: {}\".format(vocab.idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq_Model(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleDict(\n",
       "      (word_sense): ModuleList(\n",
       "        (0): Linear(in_features=512, out_features=300, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=300, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (mlp_dropout): Dropout(p=0)\n",
       "    (dimension_reduction_MLP): Linear(in_features=3072, out_features=512, bias=True)\n",
       "    (lstm): LSTM(512, 256, num_layers=2, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lstm_cell): LSTMCell(512, 256)\n",
       "    (linear): Linear(in_features=256, out_features=49036, bias=True)\n",
       "  )\n",
       "  (embed): Embedding(49036, 256, padding_idx=0)\n",
       "  (dropout): Dropout(p=0)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size = vocab.idx)\n",
    "encoder = Encoder(all_senses = all_senses, elmo_class = elmo)\n",
    "seq2seq_model = Seq2Seq_Model(encoder, decoder, vocab = vocab, all_senses = all_senses)\n",
    "\n",
    "# randomly initialize the weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)       \n",
    "seq2seq_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD_IDX: 0\n"
     ]
    }
   ],
   "source": [
    "# training hyperparameters\n",
    "optimizer = optim.Adam(seq2seq_model.parameters())\n",
    "PAD_IDX = vocab('<pad>')\n",
    "print('PAD_IDX: {}'.format(PAD_IDX))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "# turn the given definition into its index list form\n",
    "def def2idx(definition, max_length, vocab):\n",
    "    \n",
    "    # definition is given by the WN NLTK API in a string\n",
    "    def_tokens = nltk.tokenize.word_tokenize(definition.lower())\n",
    "    \n",
    "    # limit the length if too long, trim\n",
    "    if len(def_tokens) > (max_length - 2):\n",
    "        def_tokens = def_tokens[0:(max_length - 2)]\n",
    "        \n",
    "        # add the start and end symbol\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "    \n",
    "    # if the length is too short, pad\n",
    "    elif len(def_tokens) < (max_length - 2):\n",
    "        \n",
    "        # add the start and end symbol\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "        \n",
    "        pad = ['<pad>'] * (max_length - len(def_tokens))\n",
    "        def_tokens = def_tokens + pad\n",
    "        \n",
    "    else:\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "            \n",
    "    # get the index for each element in the token list\n",
    "    def_idx_list = [vocab(token) for token in def_tokens]\n",
    "    \n",
    "    return def_idx_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training function\n",
    "def train(model, train_X, train_Y, train_idx, all_senses, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # SGD\n",
    "    for idx, sentence in enumerate(train_X):\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the target word\n",
    "        word_idx = train_idx[idx]\n",
    "        word_lemma = '____' + sentence[word_idx]\n",
    "        \n",
    "        # get the target definition idx\n",
    "        # use the first correct response for now\n",
    "        response = train_Y[idx].index(1)\n",
    "        synset = all_senses[word_lemma][response]\n",
    "        definition = wn.synset(synset).definition()\n",
    "        def_idx_list = def2idx(definition, model.max_length, vocab)\n",
    "        \n",
    "        # get the encoder-decoder result\n",
    "        # (max_length, batch_size, vocab_size)\n",
    "        output = model(sentence, word_idx, def_idx_list, teacher_forcing_ratio = 0.4)\n",
    "        \n",
    "        # adjust dimension for loss calculation\n",
    "        target = torch.tensor(def_idx_list, dtype = torch.long)\n",
    "        output = output.squeeze(1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # add clip for gradient boost\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "def evaluate(model, dev_X, dev_Y, dev_idx, all_senses, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for idx, sentence in enumerate(dev_X):\n",
    "\n",
    "            # get the target word\n",
    "            word_idx = dev_idx[idx]\n",
    "            word_lemma = '____' + sentence[word_idx]\n",
    "\n",
    "            # get the target definition idx\n",
    "            # use the first correct response for now\n",
    "            response = dev_Y[idx].index(1)\n",
    "            synset = all_senses[word_lemma][response]\n",
    "            definition = wn.synset(synset).definition()\n",
    "            def_idx_list = def2idx(definition, model.max_length, vocab)\n",
    "\n",
    "            # get the encoder-decoder result\n",
    "            # (max_length, batch_size, vocab_size)\n",
    "            # turn off teacher forcing\n",
    "            output = model(sentence, word_idx, def_idx_list, teacher_forcing_ratio = 0)\n",
    "\n",
    "            # adjust dimension for loss calculation\n",
    "            target = torch.tensor(def_idx_list, dtype = torch.long)\n",
    "            output = output.squeeze(1)\n",
    "\n",
    "            loss = criterion(output, target)            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dev_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time used by each epoch\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 26s\n",
      "\tTrain Loss: 10.144 | Train PPL: 25425.560\n",
      "\t Val. Loss: 8.661 |  Val. PPL: 5770.925\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(seq2seq_model, train_X, train_Y, train_word_idx, all_senses, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(seq2seq_model, new_dev_X, new_dev_Y, new_dev_idx, all_senses, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # save the best model based on the dev set\n",
    "    '''\n",
    "    if valid_loss <= best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(seq2seq_model.state_dict(), 'best_model.pth')\n",
    "    '''\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d5ea424c7b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcsv_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcsv_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "# plot the learning curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "with open('train_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(train_losses)\n",
    "\n",
    "    \n",
    "with open('dev_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(dev_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(train_losses, ms = 4, marker = 's', label = \"Train Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(train_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(dev_losses, ms = 4, marker = 'o', label = \"Dev Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(dev_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dev_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "print('test debug')\n",
    "for test_idx, test_sen in enumerate(test_X):\n",
    "    test_lemma = '____' + test_sen[test_word_idx[test_idx]]\n",
    "    emb_length = len(all_test_senses.get(test_lemma))\n",
    "    y = len(test_Y[test_idx])\n",
    "    \n",
    "    if emb_length != y:\n",
    "        print('lemma: {}, y: {}, emb: {}'.format(test_lemma, y, emb_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "print('dev debug')\n",
    "for test_idx, test_sen in enumerate(dev_X):\n",
    "    test_lemma = '____' + test_sen[dev_word_idx[test_idx]]\n",
    "    emb_length = len(all_senses.get(test_lemma))\n",
    "    y = len(dev_Y[test_idx])\n",
    "    \n",
    "    if emb_length != y:\n",
    "        print('lemma: {}, y: {}, emb: {}'.format(test_lemma, y, emb_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# should print nothing \n",
    "print('train debug')\n",
    "for test_idx, test_sen in enumerate(train_X):\n",
    "    test_lemma = '____' + test_sen[train_word_idx[test_idx]]\n",
    "    if all_senses.get(test_lemma, 'e') == 'e':\n",
    "        print(test_lemma)\n",
    "        print(test_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "# should print nothing \n",
    "print('test debug')\n",
    "for test_idx, test_sen in enumerate(test_X):\n",
    "    test_lemma = '____' + test_sen[test_word_idx[test_idx]]\n",
    "    if all_test_senses.get(test_lemma, 'e') == 'e':\n",
    "        print(test_lemma)\n",
    "        print(test_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
