{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "from io import open\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Iterable, defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSEED = 1234\\nrandom.seed(SEED)\\ntorch.manual_seed(SEED)\\ntorch.backends.cudnn.deterministic = True\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set determinstic results\n",
    "'''\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Size of vocab: 49036\n",
      "Size of synset vocab: 117659\n",
      "Size of synset vocab in SemCor: 25947\n"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()\n",
    "\n",
    "from graph_lstm import *\n",
    "from decoder import *\n",
    "from graph2seq_model import *\n",
    "\n",
    "# get the decoder vocab\n",
    "with open('./data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    print(\"Size of vocab: {}\".format(vocab.idx))\n",
    "    \n",
    "# get the graph lstm synset vocab\n",
    "with open('./data/synset_vocab.pkl', 'rb') as f:\n",
    "    synset_vocab = pickle.load(f)\n",
    "print(\"Size of synset vocab: {}\".format(synset_vocab.idx))\n",
    "\n",
    "# get the graph lstm synset vocab that only appears in the SemCor\n",
    "with open('./data/synset_vocab_SemCor.pkl', 'rb') as f:\n",
    "    synset_vocab_SemCor = pickle.load(f)\n",
    "print(\"Size of synset vocab in SemCor: {}\".format(synset_vocab_SemCor.idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "# data parallel for the decoder\n",
    "# not working for the decoder here since we are using SGD per node\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([117659, 256])\n",
      "weight_ih_l0 torch.Size([512, 256])\n",
      "weight_hh_l0 torch.Size([512, 128])\n",
      "bias_ih_l0 torch.Size([512])\n",
      "bias_hh_l0 torch.Size([512])\n",
      "weight_ih_l0_reverse torch.Size([512, 256])\n",
      "weight_hh_l0_reverse torch.Size([512, 128])\n",
      "bias_ih_l0_reverse torch.Size([512])\n",
      "bias_hh_l0_reverse torch.Size([512])\n",
      "weight_ih_l1 torch.Size([512, 256])\n",
      "weight_hh_l1 torch.Size([512, 128])\n",
      "bias_ih_l1 torch.Size([512])\n",
      "bias_hh_l1 torch.Size([512])\n",
      "weight_ih_l1_reverse torch.Size([512, 256])\n",
      "weight_hh_l1_reverse torch.Size([512, 128])\n",
      "bias_ih_l1_reverse torch.Size([512])\n",
      "bias_hh_l1_reverse torch.Size([512])\n",
      "embedding.weight torch.Size([117659, 256])\n",
      "weight_ih torch.Size([1024, 512])\n",
      "weight_hh torch.Size([1024, 256])\n",
      "bias_ih torch.Size([1024])\n",
      "bias_hh torch.Size([1024])\n",
      "weight torch.Size([49036, 256])\n",
      "bias torch.Size([49036])\n",
      "lstm_cell.weight_ih torch.Size([1024, 512])\n",
      "lstm_cell.weight_hh torch.Size([1024, 256])\n",
      "lstm_cell.bias_ih torch.Size([1024])\n",
      "lstm_cell.bias_hh torch.Size([1024])\n",
      "linear.weight torch.Size([49036, 256])\n",
      "linear.bias torch.Size([49036])\n",
      "weight torch.Size([49036, 256])\n",
      "graph_lstm.weight_ih_l0 torch.Size([512, 256])\n",
      "graph_lstm.weight_hh_l0 torch.Size([512, 128])\n",
      "graph_lstm.bias_ih_l0 torch.Size([512])\n",
      "graph_lstm.bias_hh_l0 torch.Size([512])\n",
      "graph_lstm.weight_ih_l0_reverse torch.Size([512, 256])\n",
      "graph_lstm.weight_hh_l0_reverse torch.Size([512, 128])\n",
      "graph_lstm.bias_ih_l0_reverse torch.Size([512])\n",
      "graph_lstm.bias_hh_l0_reverse torch.Size([512])\n",
      "graph_lstm.weight_ih_l1 torch.Size([512, 256])\n",
      "graph_lstm.weight_hh_l1 torch.Size([512, 128])\n",
      "graph_lstm.bias_ih_l1 torch.Size([512])\n",
      "graph_lstm.bias_hh_l1 torch.Size([512])\n",
      "graph_lstm.weight_ih_l1_reverse torch.Size([512, 256])\n",
      "graph_lstm.weight_hh_l1_reverse torch.Size([512, 128])\n",
      "graph_lstm.bias_ih_l1_reverse torch.Size([512])\n",
      "graph_lstm.bias_hh_l1_reverse torch.Size([512])\n",
      "graph_lstm.embedding.weight torch.Size([117659, 256])\n",
      "decoder.lstm_cell.weight_ih torch.Size([1024, 512])\n",
      "decoder.lstm_cell.weight_hh torch.Size([1024, 256])\n",
      "decoder.lstm_cell.bias_ih torch.Size([1024])\n",
      "decoder.lstm_cell.bias_hh torch.Size([1024])\n",
      "decoder.linear.weight torch.Size([49036, 256])\n",
      "decoder.linear.bias torch.Size([49036])\n",
      "embed.weight torch.Size([49036, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph2Seq_Model(\n",
       "  (graph_lstm): ChildSumGraphLSTM_WordNet(\n",
       "    256, 128, num_layers=2, dropout=0.4, bidirectional=True\n",
       "    (embedding): Embedding(117659, 256)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lstm_cell): LSTMCell(512, 256)\n",
       "    (linear): Linear(in_features=256, out_features=49036, bias=True)\n",
       "  )\n",
       "  (embed): Embedding(49036, 256, padding_idx=0)\n",
       "  (dropout): Dropout(p=0.375)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some hyperparameters\n",
    "max_seq_length = 20\n",
    "decoder_hidden_size = 256\n",
    "decoder_input_size = 512\n",
    "depth = 2\n",
    "\n",
    "# please check the emb2seq_parallel_train.py for CUDA parallel version\n",
    "graph_lstm = ChildSumGraphLSTM_WordNet(synset_vocab = synset_vocab, input_size = 256, hidden_size = 128, num_layers = 2, bidirectional = True, bias = True, dropout = 0.4)\n",
    "decoder = Decoder(vocab_size = vocab.idx, max_seq_length = max_seq_length, hidden_size = decoder_hidden_size, input_size = decoder_input_size)\n",
    "graph2seq_model = Graph2Seq_Model(graph_lstm, depth, decoder, vocab = vocab, max_seq_length = max_seq_length, decoder_hidden_size = decoder_hidden_size)\n",
    "graph2seq_model.to(device)\n",
    "\n",
    "# randomly initialize the weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.shape)\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "graph2seq_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD_IDX: 0\n"
     ]
    }
   ],
   "source": [
    "# training hyperparameters\n",
    "optimizer = optim.Adam(graph2seq_model.parameters())\n",
    "PAD_IDX = vocab('<pad>')\n",
    "print('PAD_IDX: {}'.format(PAD_IDX))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "# turn the given definition into its index list form\n",
    "def def2idx(definition, max_length, vocab):\n",
    "    \n",
    "    # definition is given by the WN NLTK API in a string\n",
    "    def_tokens = nltk.tokenize.word_tokenize(definition.lower())\n",
    "    \n",
    "    # limit the length if too long, trim\n",
    "    if len(def_tokens) > (max_length - 2):\n",
    "        def_tokens = def_tokens[0:(max_length - 2)]\n",
    "        \n",
    "        # add the start and end symbol\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "    \n",
    "    # if the length is too short, pad\n",
    "    elif len(def_tokens) < (max_length - 2):\n",
    "        \n",
    "        # add the start and end symbol\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "        \n",
    "        pad = ['<pad>'] * (max_length - len(def_tokens))\n",
    "        def_tokens = def_tokens + pad\n",
    "        \n",
    "    else:\n",
    "        def_tokens = ['<start>'] + def_tokens + ['<end>']\n",
    "            \n",
    "    # get the index for each element in the token list\n",
    "    def_idx_list = [vocab(token) for token in def_tokens]\n",
    "    \n",
    "    return def_idx_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training function\n",
    "# pretrain on the synsets appeared in the SemCor\n",
    "small_size = 10\n",
    "def train(model, optimizer, synset_vocab_SemCor, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    synset_num = small_size\n",
    "    \n",
    "    # visualize results\n",
    "    all_definitions = []\n",
    "    all_sentence_result = []\n",
    "    \n",
    "    # for idx in range(synset_vocab_SemCor.idx):\n",
    "    for idx in range(small_size):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # get the synset and definition\n",
    "        synset = synset_vocab_SemCor.idx2word.get(idx).replace('__', '.')\n",
    "        definition = wn.synset(synset).definition()\n",
    "        # print(synset, definition)\n",
    "        all_definitions.append(definition)\n",
    "        \n",
    "        # convert to def index list\n",
    "        def_idx_list = def2idx(definition, model.max_length, vocab)\n",
    "\n",
    "        # get the graph-decoder result\n",
    "        # (self.max_length, batch_size, vocab_size) where batch_size == 1\n",
    "        output, result = model(synset, def_idx_list, teacher_forcing_ratio = 0.4)\n",
    "        all_sentence_result.append(result)\n",
    "\n",
    "        # adjust dimension for loss calculation\n",
    "        output = output.squeeze(1)\n",
    "        target = torch.tensor(def_idx_list, dtype = torch.long).to(device)\n",
    "        # print(target, definition)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "\n",
    "        # add clip for gradient boost\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "                \n",
    "    return epoch_loss / synset_num, all_sentence_result, all_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time used by each epoch\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "# arrange the result back to literal readable form\n",
    "def arrange_result(all_sentence_result):\n",
    "    arranged_all_sentence_result = []\n",
    "    \n",
    "    for result in all_sentence_result:\n",
    "        arranged_results = ''\n",
    "        \n",
    "        for word_idx in result:\n",
    "            w = ' '+ vocab.idx2word.get(int(word_idx))\n",
    "            arranged_results += w\n",
    "            \n",
    "        arranged_all_sentence_result.append(arranged_results)\n",
    "    return arranged_all_sentence_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function\n",
    "# write the results to the file, with the ground-truth\n",
    "def write_result_to_file(arranged_all_sentence_result, all_definition):\n",
    "    with open('result.txt', 'w') as f:\n",
    "        for idx, arranged_results in enumerate(arranged_all_sentence_result):\n",
    "            f.write(\"sentence {}\\n\".format(idx))\n",
    "            f.write(\"%s\\n\" % all_definition[idx])\n",
    "            f.write(\"%s\\n\" % arranged_results)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 39s\n",
      "\tTrain Loss: 10.733 | Train PPL: 45832.031\n",
      "Epoch: 02 | Time: 0m 37s\n",
      "\tTrain Loss: 9.710 | Train PPL: 16478.884\n",
      "Epoch: 03 | Time: 0m 39s\n",
      "\tTrain Loss: 6.568 | Train PPL: 712.014\n",
      "Epoch: 04 | Time: 0m 38s\n",
      "\tTrain Loss: 4.763 | Train PPL: 117.064\n",
      "Epoch: 05 | Time: 0m 35s\n",
      "\tTrain Loss: 4.065 | Train PPL:  58.293\n"
     ]
    }
   ],
   "source": [
    "# train \n",
    "import time\n",
    "\n",
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "best_train_loss = float('inf')\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, all_sentence_result, all_definitions = train(graph2seq_model, optimizer, synset_vocab_SemCor, criterion, CLIP)\n",
    "    train_losses.append(train_loss)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # visualize the results\n",
    "    arranged_all_sentence_result = arrange_result(all_sentence_result)\n",
    "            \n",
    "    # save the best model based on the train set\n",
    "    # since the nature of our pretrain without context, dev is useless\n",
    "    if train_loss <= best_train_loss:\n",
    "\n",
    "        best_train_loss = train_loss\n",
    "        torch.save(graph2seq_model.state_dict(), 'graph2seq_best_model.pth')\n",
    "        \n",
    "        # record the result\n",
    "        write_result_to_file(arranged_all_sentence_result, all_definitions)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "with open('train_loss.tsv', mode = 'w') as loss_file:\n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(train_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(train_losses, ms = 4, marker = 's', label = \"Train Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"CrossEntropy Loss\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
